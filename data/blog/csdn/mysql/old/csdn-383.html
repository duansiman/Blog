\n\n<p><a target="_blank" href="http://blog.csdn.net/l1028386804/article/details/52902027">转载请注明出处：http://blog.csdn.net/l1028386804/article/details/52902027</a><br />&#13;\n</p>&#13;\n<h3>一、 基本环境介绍及基本环境配置</h3>&#13;\n<p>节点1： node1.hulala.com      192.168.1.35     centos6.5_64    添加8G新硬盘<br />&#13;\n节点2： node2.hulala.com      192.168.1.36     centos6.5_64    添加8G新硬盘<br />&#13;\nvip   192.168.1.39  <br />&#13;\n<br />&#13;\n节点1与节点2均需配置<br />&#13;\n修改主机名：</p>&#13;\n<p></p>&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_1_8847394" name="code" class="plain">vim /etc/sysconfig/network\nHOSTNAME=node1.hulala.com</pre>配置hosts解析：<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_2_831479" name="code" class="plain">vim /etc/hosts\n192.168.1.35 node1.hulala.com node1\n192.168.1.36 node2.hulala.com node2 </pre>同步系统时间：<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_3_7309468" name="code" class="plain">ntpdate cn.pool.ntp.org</pre>关闭防火墙与SELINUX<br />&#13;\n<p></p>&#13;\n<p></p>&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_4_515052" name="code" class="plain">service iptables stop\nchkconfig iptables off\ncat /etc/sysconfig/selinux\nSELINUX=disabled</pre>以上配置在两个节点都需要配置，配置完成之后重启两个节点&#13;\n<p></p>&#13;\n<h3>二:配置ssh互信<br />&#13;\n</h3>&#13;\n<p></p>&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_5_8944888" name="code" class="plain">[root@node1～]#ssh-keygen -t rsa -b 1024\n[root@node1～]#ssh-copy-id root@192.168.1.36\n[root@node2～]#ssh-keygen -t rsa -b 1024\n[root@node2～]#ssh-copy-id root@192.168.1.35</pre>&#13;\n<p></p>&#13;\n<h3>三：DRBD的安装与配置（node1和node2执行相同操作）</h3>&#13;\n<p></p>&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_6_8246776" name="code" class="plain">[root@node1～]#wget -c http://elrepo.org/linux/elrepo/el6/x86_64/RPMS/drbd84-utils-8.4.2-1.el6.elrepo.x86_64.rpm\n[root@node1～]#wget -c http://elrepo.org/linux/elrepo/el6/x86_64/RPMS/kmod-drbd84-8.4.2-1.el6_3.elrepo.x86_64.rpm\n[root@node1～]#rpm -ivh *.rpm</pre>获取一个sha1值做为shared-secret<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_7_8977125" name="code" class="plain">[root@node1～]#sha1sum /etc/drbd.conf\n8a6c5f3c21b84c66049456d34b4c4980468bcfb3 /etc/drbd.conf</pre>创建并编辑资源配置文件:/etc/drbd.d/dbcluster.res<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_8_9915216" name="code" class="plain">[root@node1～]# vim /etc/drbd.d/dbcluster.res\nresource dbcluster {\n protocol C;\n net {\n cram-hmac-alg sha1;\n shared-secret "8a6c5f3c21b84c66049456d34b4c4980468bcfb3";\n after-sb-0pri discard-zero-changes;\nafter-sb-1pri discard-secondary;\n after-sb-2pri disconnect;\n rr-conflict disconnect;\n }\n device /dev/drbd0;\n disk /dev/sdb1;\nmeta-disk internal;\n on node1.hulala.com {\n address 192.168.1.35:7789;\n }\n on node2.hulala.com {\n address 192.168.1.36:7789;\n }\n}</pre>以上配置所用参数说明:<br />&#13;\nRESOURCE: 资源名称<br />&#13;\nPROTOCOL: 使用协议”C”表示”同步的”,即收到远程的写入确认之后,则认为写入完成.<br />&#13;\nNET: 两个节点的SHA1 key是一样的<br />&#13;\nafter-sb-0pri : “Split Brain”发生时且没有数据变更,两节点之间正常连接<br />&#13;\nafter-sb-1pri : 如果有数据变更,则放弃辅设备数据,并且从主设备同步<br />&#13;\nrr-conflict: 假如前面的设置不能应用,并且drbd系统有角色冲突的话,系统自动断开节点间连接<br />&#13;\nMETA-DISK: Meta data保存在同一个磁盘(sdb1)<br />&#13;\nON &lt;NODE&gt;: 组成集群的节点<br />&#13;\n将DRBD配置拷贝到node机器:<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_9_9217105" name="code" class="plain">[root@node1～]#scp /etc/drbd.d/dbcluster.res root@192.168.1.36:/etc/drbd.d/</pre>创建资源及文件系统:<br />&#13;\n创建分区(未格式化过)<br />&#13;\n在node1和node2上创建LVM分区：<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_10_6915810" name="code" class="plain">[#root@node1～]fdisk /dev/sdb</pre>在node1和node2上给资源(dbcluster)创建meta data：<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_11_4298872" name="code" class="plain">[root@node1～drbd]#drbdadm create-md dbcluster</pre>激活资源（node1和node2都得查看）<br />&#13;\n– 首先确保drbd module已经加载<br />&#13;\n查看是否加载:<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_12_45730" name="code" class="plain"># lsmod | grep drbd</pre>若未加载,则需加载:<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_13_2727926" name="code" class="plain"># modprobe drbd\n# lsmod | grep drbd\ndrbd 317261 0\nlibcrc32c 1246 1 drbd</pre>– 启动drbd后台进程:<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_14_5617864" name="code" class="plain">[root@node1 drbd]# drbdadm up dbcluster\n[root@node2 drbd]# drbdadm up dbcluster</pre>查看（node1和node2）drbd状态:<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_15_6904620" name="code" class="plain">[root@node2 drbd]# /etc/init.d/drbd status\nGIT-hash: 7ad5f850d711223713d6dcadc3dd48860321070c build by dag@Build64R6, 2016-10-23 08:16:10\nm:res cs ro ds p mounted fstype\n0:dbcluster Connected Secondary/Secondary Inconsistent/Inconsistent C</pre>从上面的信息可以看到,DRBD服务已经在两台机器上运行,但任何一台机器都不是主机器(“primary” host),因此无法访问到资源(block device).<br />&#13;\n开始同步：<br />&#13;\n仅在主节点操作(这里为node1)<br />&#13;\n<p></p>&#13;\n<p></p>&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_16_110204" name="code" class="plain">[root@node1 drbd]# drbdadm — –overwrite-data-of-peer primary dbcluster</pre>查看同步状态:<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_17_8540040" name="code" class="plain">[root@node1 drbd.d]# cat /proc/drbd\nversion: 8.4.2 (api:1/proto:86-101)\nGIT-hash: 7ad5f850d711223713d6dcadc3dd48860321070c build by dag@Build64R6, 2016-10-23 08:16:10\n 0: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r—–\n ns:8297248 nr:0 dw:0 dr:8297912 al:0 bm:507 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0</pre>上面的输出结果的一些说明:<br />&#13;\ncs (connection state): 网络连接状态<br />&#13;\nro (roles): 节点的角色(本节点的角色首先显示)<br />&#13;\nds (disk states):硬盘的状态<br />&#13;\n复制协议: A, B or C(本配置是C)<br />&#13;\n看到drbd状态为”cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate”即表示同步结束.<br />&#13;\n也可以这样查看drbd状态:<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_18_6969876" name="code" class="plain">[root@centos193 drbd]# drbd-overview\n 0:dbcluster/0 Connected Secondary/Primary UpToDate/UpToDate C r—–</pre>创建文件系统：<br />&#13;\n在主节点(Node1)创建文件系统:<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_19_5191970" name="code" class="plain">[root@node1 drbd]# mkfs -t ext4 /dev/drbd0\nmke2fs 1.41.12 (17-May-2010)\nFilesystem label=\nOS type: Linux\nBlock size=4096 (log=2)\n…….\n180 days, whichever comes first. Use tune2fs -c or -i to override.</pre>注:没必要在辅节点(Node2)做同样的操作,因为DRBD会处理原始磁盘数据的同步.<br />&#13;\n另外,我们也不需要将这个DRBD系统挂载到任何一台机器(当然安装MySQL的时候需要临时挂载来安装MySQL),因为集群管理软件会处理.还有要确保复制的文件系统仅仅挂载在Active的主服务器上.<br />&#13;\n<p></p>&#13;\n<h3>四：mysql的安装</h3>&#13;\n<p>MySQL的安装也可以参见博文《<a target="_blank" href="http://blog.csdn.net/l1028386804/article/details/52181103">MySQL之——CentOS6.5 编译安装MySQL5.6.16</a>》<br />&#13;\n</p>&#13;\n<p>1,在node1和node2节点安装mysql:<br />&#13;\n</p>&#13;\n<p></p>&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_20_6478726" name="code" class="plain">yum install mysql.sh -y</pre>2.node1和node2都操作停止mysql服务<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_21_6129279" name="code" class="plain">[root@node1～]# service mysql stop\nShutting down MySQL. [ OK ]</pre>3.node1和node2都操作创建数据库目录并将该目录权限属主修改为mysql<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_22_6543982" name="code" class="plain">[root@host1 /]# mkdir -p /mysql/data\n[root@host1 /]# chown -R mysql:mysql /mysql</pre>4，关闭mysql临时挂载DRBD文件系统到主节点(Node1)<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_23_3337616" name="code" class="plain">[root@node1 ~]# mount /dev/drbd0 /mysql/</pre>5.node1和node2都操作修改my.cnf文件修改<br />&#13;\n在[mysqld]下添加新的数据存放路径<br />&#13;\n<p></p>&#13;\n<p></p>&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_24_4591352" name="code" class="plain">datadir=/mysql/data</pre>6.将默认的数据路径下的所有文件和目录cp到新的目录下（node2不用操作）<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_25_8909750" name="code" class="plain">[root@host1 mysql]#cd /var/lib/mysql\n[root@host1 mysql]#cp -R csdn-100.html csdn-101.html csdn-102.html csdn-103.html csdn-104.html csdn-105.html csdn-106.html csdn-107.html csdn-108.html csdn-109.html csdn-10.html csdn-110.html csdn-111.html csdn-112.html csdn-113.html csdn-114.html csdn-115.html csdn-116.html csdn-117.html csdn-118.html csdn-119.html csdn-11.html csdn-120.html csdn-121.html csdn-122.html csdn-123.html csdn-124.html csdn-125.html csdn-126.html csdn-127.html csdn-128.html csdn-129.html csdn-12.html csdn-130.html csdn-131.html csdn-132.html csdn-133.html csdn-134.html csdn-135.html csdn-136.html csdn-137.html csdn-138.html csdn-139.html csdn-13.html csdn-140.html csdn-141.html csdn-142.html csdn-143.html csdn-144.html csdn-145.html csdn-146.html csdn-147.html csdn-148.html csdn-149.html csdn-14.html csdn-150.html csdn-151.html csdn-152.html csdn-153.html csdn-154.html csdn-155.html csdn-156.html csdn-157.html csdn-158.html csdn-159.html csdn-15.html csdn-160.html csdn-161.html csdn-162.html csdn-163.html csdn-164.html csdn-165.html csdn-166.html csdn-167.html csdn-168.html csdn-169.html csdn-16.html csdn-170.html csdn-171.html csdn-172.html csdn-173.html csdn-174.html csdn-175.html csdn-176.html csdn-177.html csdn-178.html csdn-179.html csdn-17.html csdn-180.html csdn-181.html csdn-182.html csdn-183.html csdn-184.html csdn-185.html csdn-186.html csdn-187.html csdn-188.html csdn-189.html csdn-18.html csdn-190.html csdn-191.html csdn-192.html csdn-193.html csdn-194.html csdn-195.html csdn-196.html csdn-197.html csdn-198.html csdn-199.html csdn-19.html csdn-1.html csdn-200.html csdn-201.html csdn-202.html csdn-203.html csdn-204.html csdn-205.html csdn-206.html csdn-207.html csdn-208.html csdn-209.html csdn-20.html csdn-210.html csdn-211.html csdn-212.html csdn-213.html csdn-214.html csdn-215.html csdn-216.html csdn-217.html csdn-218.html csdn-219.html csdn-21.html csdn-220.html csdn-221.html csdn-222.html csdn-223.html csdn-224.html csdn-225.html csdn-226.html csdn-227.html csdn-228.html csdn-229.html csdn-22.html csdn-230.html csdn-231.html csdn-232.html csdn-233.html csdn-234.html csdn-235.html csdn-236.html csdn-237.html csdn-238.html csdn-239.html csdn-23.html csdn-240.html csdn-241.html csdn-242.html csdn-243.html csdn-244.html csdn-245.html csdn-246.html csdn-247.html csdn-248.html csdn-249.html csdn-24.html csdn-250.html csdn-251.html csdn-252.html csdn-253.html csdn-254.html csdn-255.html csdn-256.html csdn-257.html csdn-258.html csdn-259.html csdn-25.html csdn-260.html csdn-261.html csdn-262.html csdn-263.html csdn-264.html csdn-265.html csdn-266.html csdn-267.html csdn-268.html csdn-269.html csdn-26.html csdn-270.html csdn-271.html csdn-272.html csdn-273.html csdn-274.html csdn-275.html csdn-276.html csdn-277.html csdn-278.html csdn-279.html csdn-27.html csdn-280.html csdn-281.html csdn-282.html csdn-283.html csdn-284.html csdn-285.html csdn-286.html csdn-287.html csdn-288.html csdn-289.html csdn-28.html csdn-290.html csdn-291.html csdn-292.html csdn-293.html csdn-294.html csdn-295.html csdn-296.html csdn-297.html csdn-298.html csdn-299.html csdn-29.html csdn-2.html csdn-300.html csdn-301.html csdn-302.html csdn-303.html csdn-304.html csdn-305.html csdn-306.html csdn-307.html csdn-308.html csdn-309.html csdn-30.html csdn-310.html csdn-311.html csdn-312.html csdn-313.html csdn-314.html csdn-315.html csdn-316.html csdn-317.html csdn-318.html csdn-319.html csdn-31.html csdn-320.html csdn-321.html csdn-322.html csdn-323.html csdn-324.html csdn-325.html csdn-326.html csdn-327.html csdn-328.html csdn-329.html csdn-32.html csdn-330.html csdn-331.html csdn-332.html csdn-333.html csdn-334.html csdn-335.html csdn-336.html csdn-337.html csdn-338.html csdn-339.html csdn-33.html csdn-340.html csdn-341.html csdn-342.html csdn-343.html csdn-344.html csdn-345.html csdn-346.html csdn-347.html csdn-348.html csdn-349.html csdn-34.html csdn-350.html csdn-351.html csdn-352.html csdn-353.html csdn-354.html csdn-355.html csdn-356.html csdn-357.html csdn-358.html csdn-359.html csdn-35.html csdn-360.html csdn-361.html csdn-362.html csdn-363.html csdn-364.html csdn-365.html csdn-366.html csdn-367.html csdn-368.html csdn-369.html csdn-36.html csdn-370.html csdn-371.html csdn-372.html csdn-373.html csdn-374.html csdn-375.html csdn-376.html csdn-377.html csdn-378.html csdn-379.html csdn-37.html csdn-380.html csdn-381.html csdn-382.html csdn-38.html csdn-39.html csdn-3.html csdn-40.html csdn-41.html csdn-42.html csdn-43.html csdn-44.html csdn-45.html csdn-46.html csdn-47.html csdn-48.html csdn-49.html csdn-4.html csdn-50.html csdn-51.html csdn-52.html csdn-53.html csdn-54.html csdn-55.html csdn-56.html csdn-57.html csdn-58.html csdn-59.html csdn-5.html csdn-60.html csdn-61.html csdn-62.html csdn-63.html csdn-64.html csdn-65.html csdn-66.html csdn-67.html csdn-68.html csdn-69.html csdn-6.html csdn-70.html csdn-71.html csdn-72.html csdn-73.html csdn-74.html csdn-75.html csdn-76.html csdn-77.html csdn-78.html csdn-79.html csdn-7.html csdn-80.html csdn-81.html csdn-82.html csdn-83.html csdn-84.html csdn-85.html csdn-86.html csdn-87.html csdn-88.html csdn-89.html csdn-8.html csdn-90.html csdn-91.html csdn-92.html csdn-93.html csdn-94.html csdn-95.html csdn-96.html csdn-97.html csdn-98.html csdn-99.html csdn-9.html mysql.sh /mysql/data/</pre>node1和node2都操作这里注意copy过去的目录权限属主需要修改为mysql,这里直接修改mysql目录即可.<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_26_2671742" name="code" class="plain">[root@host1 mysql]# chown -R mysql:mysql /mysql</pre>7.启动node1上的mysql进行登陆测试<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_27_1101579" name="code" class="plain">[root@host1 mysql]# mysql</pre>8.在节点Node1卸载DRBD文件系统<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_28_8800283" name="code" class="plain">[root@node1 ~]# umount /var/lib/mysql_drbd\n[root@node1 ~]# drbdadm secondary dbcluster</pre>9.将DRBD文件系统挂载节点Node2<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_29_1690223" name="code" class="plain">[root@node2 ~]# drbdadm primary dbcluster\n[root@node2 ~]# mount /dev/drbd0 /mysql/</pre>10.节点Node2上配置MySQL并测试<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_30_2628314" name="code" class="plain">[root@node1 ~]# scp node2:/etc/my.cnf /etc/my.cnf\n[root@node2 ~]# chown mysql /etc/my.cnf\n[root@node2 ~]# chmod 644 /etc/my.cnf</pre>11. node2上做mysql登陆测试<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_31_1722460" name="code" class="plain">[root@node2 ~]# mysql</pre>12.在Node2上卸载DRBD文件系统,交由集群管理软件Pacemaker来管理<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_32_4122030" name="code" class="plain">[root@node2～]# umount /var/lib/mysql_drbd\n[root@node2～]# drbdadm secondary dbcluster\n[root@node2～]# drbd-overview\n 0:dbcluster/0 Connected Secondary/Secondary UpToDate/UpToDate C r—–\n[root@node2～]#</pre>&#13;\n<p></p>&#13;\n<h3>五：Corosync和Pacemaker的安装配置（node1和node2都需安装）</h3>&#13;\n<p>安装Pacemaker必须依赖:<br />&#13;\n</p>&#13;\n<p></p>&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_33_4852379" name="code" class="plain">[root@node1～]#yum -y install automake autoconf libtool-ltdl-devel pkgconfig python glib2-devel libxml2-devel libxslt-devel python-devel gcc-c++ bzip2-devel gnutls-devel pam-devel libqb-devel</pre>安装Cluster Stack依赖:<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_34_9694164" name="code" class="plain">[root@node1～]yum -y install clusterlib-devel corosynclib-devel</pre>安装Pacemaker可选依赖:<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_35_2584104" name="code" class="plain">[root@node1～]yum -y install ncurses-devel openssl-devel cluster-glue-libs-devel docbook-style-xsl</pre>Pacemaker安装:<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_36_5789687" name="code" class="plain">[root@node1～]yum -y install pacemaker</pre>crmsh安装:<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_37_6171371" name="code" class="plain">[root@node1～]wget http://download.opensuse.org/repositories/network:/ha-clustering:/Stable/CentOS_CentOS-6/network:ha-clustering:Stable.repo\n[root@node1～]yum -y install crmsh</pre>1，配置corosync<br />&#13;\nCorosync Key<br />&#13;\n– 生成节点间安全通信的key:<br />&#13;\n<p></p>&#13;\n<p></p>&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_38_4077820" name="code" class="plain">[root@node1～]# corosync-keygen</pre><pre code_snippet_id="1944543" snippet_file_name="blog_20161023_39_6203608" name="code" class="plain">– 将authkey拷贝到node2节点(保持authkey的权限为400):\n[root@node～]# scp /etc/corosync/authkey node2:/etc/corosync/\n2，[root@node1～]# cp /etc/corosync/corosync.conf.example /etc/corosync/corosync.conf</pre>编辑/etc/corosync/corosync.conf:<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_40_8046771" name="code" class="plain"># Please read the corosync.conf.5 manual page\ncompatibility: whitetank\naisexec {\n user: root\n group: root\n}\ntotem {\n version: 2\nsecauth: off\nthreads: 0\ninterface {\nringnumber: 0\nbindnetaddr: 192.168.1.0\nmcastaddr: 226.94.1.1\nmcastport: 4000\nttl: 1\n}\n}\nlogging {\nfileline: off\nto_stderr: no\nto_logfile: yes\nto_syslog: yes\nlogfile: /var/log/cluster/corosync.log\ndebug: off\ntimestamp: on\nlogger_subsys {\nsubsys: AMF\ndebug: off\n}\n}\namf {\nmode: disabled\n}</pre>– 创建并编辑/etc/corosync/service.d/pcmk,添加”pacemaker”服务<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_41_8777119" name="code" class="plain">[root@node1～]# cat /etc/corosync/service.d/pcmk\nservice {\n\t# Load the Pacemaker Cluster Resource Manager\n\tname: pacemaker\n\tver: 1\n}</pre>将上面两个配置文件拷贝到另一节点<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_42_6683568" name="code" class="plain">[root@node1]# scp /etc/corosync/corosync.conf node2:/etc/corosync/corosync.conf\n[root@node1]# scp /etc/corosync/service.d/pcmk node2:/etc/corosync/service.d/pcmk</pre>3，启动corosync和Pacemaker<br />&#13;\n 分别在两个节点上启动corosync并检查.<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_43_5985457" name="code" class="plain">[root@node1]# /etc/init.d/corosync start\nStarting Corosync Cluster Engine (corosync): [ OK ]\n[root@node1～]# corosync-cfgtool -s\nPrinting ring status.\nLocal node ID -1123964736\nRING ID 0\nid = 192.168.1.189\nstatus = ring 0 active with no faults\n[root@node2]# /etc/init.d/corosync start\nStarting Corosync Cluster Engine (corosync): [ OK ]</pre>– 在两节点上分别启动Pacemaker:<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_44_9191040" name="code" class="plain">[root@node1~]# /etc/init.d/pacemaker start\nStarting Pacemaker Cluster Manager: [ OK ]\n[root@node2~]# /etc/init.d/pacemaker start\nStarting Pacemaker Cluster Manager: </pre>&#13;\n<p></p>&#13;\n<h3>六、资源配置</h3>&#13;\n<p><br />&#13;\n配置资源及约束                 <br />&#13;\n配置默认属性<br />&#13;\n查看已存在的配置:</p>&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_46_9431020" name="code" class="plain">[root@node1 ~]# crm configure property stonith-enabled=false\n[root@node1 ~]# crm_verify -L</pre>禁止STONITH错误:<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_46_9431020" name="code" class="plain">[root@node1 ~]# crm configure property stonith-enabled=false\n[root@node1 ~]# crm_verify -L</pre>让集群忽略Quorum:<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_47_7129726" name="code" class="plain">[root@node1~]# crm configure property no-quorum-policy=ignore</pre>防止资源在恢复之后移动:<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_48_1971513" name="code" class="plain">[root@node1~]# crm configure rsc_defaults resource-stickiness=100</pre>设置操作的默认超时:<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_49_2386217" name="code" class="plain">[root@node1~]# crm configure property default-action-timeout="180s"</pre>设置默认的启动失败是否为致命的:<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_50_9736257" name="code" class="plain">[root@node1~]# crm configure property start-failure-is-fatal="false"</pre>配置DRBD资源<br />&#13;\n– 配置之前先停止DRBD:<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_51_9038146" name="code" class="plain">[root@node1~]# /etc/init.d/drbd stop\n[root@node2~]# /etc/init.d/drbd stop</pre>– 配置DRBD资源:<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_52_3879933" name="code" class="plain">[root@node1~]# crm configure\ncrm(live)configure# primitive p_drbd_mysql ocf:linbit:drbd params drbd_resource="dbcluster" op monitor interval="15s" op start timeout="240s" op stop timeout="100s"</pre>– 配置DRBD资源主从关系(定义只有一个Master节点):<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_53_1545619" name="code" class="plain">crm(live)configure# ms ms_drbd_mysql p_drbd_mysql meta master-max="1" master-node-max="1" clone-max="2" clone-node-max="1" notify="true"</pre>– 配置文件系统资源,定义挂载点(mount point):<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_54_8023607" name="code" class="plain">crm(live)configure# primitive p_fs_mysql ocf:heartbeat:Filesystem params device="/dev/drbd0" directory="/var/lib/mysql_drbd/" fstype="ext4"</pre>配置VIP资源<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_55_3737446" name="code" class="plain">crm(live)configure# primitive p_ip_mysql ocf:heartbeat:IPaddr2 params ip="192.168.1.39" cidr_netmask="24" op monitor interval="30s"</pre>配置MySQL资源<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_56_1087488" name="code" class="plain">crm(live)configure# primitive p_mysql lsb:mysql op monitor interval="20s" timeout="30s" op start interval="0" timeout="180s" op stop interval="0" timeout="240s"</pre>&#13;\n<p></p>&#13;\n<h3>七、组资源和约束</h3>&#13;\n<p>通过”组”确保DRBD,MySQL和VIP是在同一个节点(Master)并且确定资源的启动/停止顺序.<br />&#13;\n</p>&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_57_8470548" name="code" class="plain">启动: p_fs_mysql–&gt;p_ip_mysql-&gt;p_mysql\n停止: p_mysql–&gt;p_ip_mysql–&gt;p_fs_mysql</pre><pre code_snippet_id="1944543" snippet_file_name="blog_20161023_58_596337" name="code" class="plain">crm(live)configure# group g_mysql p_fs_mysql p_ip_mysql p_mysql</pre>组group_mysql永远只在Master节点:<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_59_1534428" name="code" class="plain">crm(live)configure# colocation c_mysql_on_drbd inf: g_mysql ms_drbd_mysql:Master</pre>MySQL的启动永远是在DRBD Master之后:<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_60_6376214" name="code" class="plain">crm(live)configure# order o_drbd_before_mysql inf: ms_drbd_mysql:promote g_mysql:start</pre>配置检查和提交<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_61_3726255" name="code" class="plain">crm(live)configure# verify\ncrm(live)configure# commit\ncrm(live)configure# quit</pre>查看集群状态和failover测试<br />&#13;\n状态查看:<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_62_3028144" name="code" class="plain">[root@node1 mysql]# crm_mon -1r</pre>Failover测试:<br />&#13;\n将Node1设置为Standby状态<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_63_8775002" name="code" class="plain">[root@node1 ~]# crm node standby</pre>过几分钟查看集群状态(若切换成功,则看到如下状态):<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_64_3093401" name="code" class="plain">[root@node1 ~]# crm status</pre>将Node1恢复online状态:<br />&#13;\n<pre code_snippet_id="1944543" snippet_file_name="blog_20161023_65_443442" name="code" class="plain">[root@node1 mysql]# crm node online\n[root@node1 mysql]# crm status</pre>&#13;\n<p></p>&#13;\n &#13;\n
